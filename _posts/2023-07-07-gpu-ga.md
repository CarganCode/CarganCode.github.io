---
layout: post
title: GPU Accelerated Genetic Algorithms
description: Evolution at speed
date: 2023-07-07
type: code
author: Tim Cargan
head-short: true
head-img: /images/lion.jpg
categories: PhD
tags: PhD, Code
usemathjax: True
---

I have started using Genetic Algorithms (GA) in my PhD research.
They are a very effect and simple tool used to solve optimisation problems.
Using a "natural selection" like proses to explore the solution space they can quickly arrive at a good candidate solution.
The nice thing about GAs is that they are very easy to implement.
However, a basic implementation can be a bit slow.

In deep learning (DL) to speed up the computation we make heavy use of GPUs (and it some cases TPUs).
A lot of work has been doing DL with JAX. parallelise
If you don't know, JAX is an autograd and XLA library for python. 
It allows you to write python code with the numpy API that can be compiled, vectorised, and run on GPUs.
Inspired by a [blog post](https://willwhitney.com/parallel-training-jax.html) from Will Whitney outlining a cool way to use `jax.vmap` to train multiple models at once. 
I asked myself, could I use the same technique to speed up my GA and run it on the GPU? 
It turns out, with a few minor caveats, you can! Check out the code on my [github](https://github.com)

<!--more-->
<!-- ##### TOC -->
<!-- The rest of this blog post is structured as follows: -->
- What is a GA?
    - A quick overview of what a GA is and how it works
- SIMD?
- How it works?


## What is a GA?
A quick overview of what a GA is and how it works.

The GA is inspired by the biologic method of evolution through natural selection, where fitter individuals are more likely to pass on their genetic material.
There is an initial population of $$N$$ candidates.
During each iteration of the GA a new population is created by combining and mutating the existing members based on their fitness.
An iteration of the algorithm is analogous to a generation.

Each candidate has genetic representation, a set of "chromosomes" that make up a solution. 
A basic encoding is simply a binary string comprised of 1s and 0s for each solution variable. 
We also need a fitness function that can take a solution representation and evaluate how good it is. 
It will return a scalar value (a single number) to represent how good the solution is, lower is normally better.

With search-based optimisation methods there is a trade-off between exploration and exploitation. 
For the same "budget," we can either _explore_ more areas of the solution space in the hope of finding improvement;
 or we can _exploit_ the local optima (goodness) we have found and try and make it better. 
GAs can strike a good balance in the trade-off between exploration and exploitation. 

### Crossover and Mutation 
This is how new members (candidate solutions) are added to the population. 
The processes mimics that of reproduction, two members are selected and exchange _genetic material_ and create a new _child_ solution. 
We then apply a mutate operator the new _child_.
The crossover is how we exploit the search space while mutation drives the exploration.

In the case of the basic binary string encoding. 
The crossover operator just randomly chooses the value of each chromosome from each of the two parents.

```python
Candidate = list[bool]
def crossover(p1:Candidate, p2:Candidate, threshold:float=0.5) -> Candidate:
    c = p1[:] # Copy
    for i in range(len(p1)):
        if uniform(0, 1) < threshold:
            c[i] = p2[i]
    return c
```

While the mutate operator can simply randomly flip bits.

```python
def mutate(s: Candidate, mutate_rate) -> Candidate
    for i in range(len(s))
         if uniform(0, 1) < mutate_rate
            s[i] = not s[i]
    return s
```

Having taught undergrads GAs for a few years now it's always amusing how invested some of them get to the parent-child analogy, on more than one occasion I have seen code along the lines of `crossover(mum, dad)`.

### Parent Selection
This is where the section part of the "natural selection" bit happens.
In a typical GA, parent selection is done where each candidate is given a probability proportional to its fitness.
That is to say the more fit an individual, the more likely it is to be selected to be a parent.
Note that the two parents could be the same.

However there are alternative methods, for example each generation only the best $$n\%$$ of solutions in the population can have children and then are sampled uniformly.
In fact there are a lot of variations that can be made to the GA.
You can play with the parent selection criteria, scale the gene selection based on fitness, have the mutation rate drop with each generation etc.

Bellow is a basic implementation that just takes the best half of the population and randomly samples from that to create a new generation.
It also keeps the best half of the population from the previous generation.

```python
population = init_pop()
pop_size = len(population)
mutate_rate = 0.3

for n in NUM_GENERATIONS:
    fitness = [fit_fn(p) for p in population]
    best_half = population[fitness.argsort()[:pop_size / 2 ]]

    new_sols = []
    for _ in range(pop_size / 2):
        p1, p2 = randint(pop_size / 2), randint(pop_size / 2)
        child = mutate(crossover(best_half[p1], best_half[p2]), mutate_rate)
        new_sols.append(child) 

    population = best_half + new_sols
```

## SIMD

Have you ever wondered why deep learning runs on GPUs? 
Itâ€™s all thanks to SIMD.
SIMD stands for "single instruction, multiple data" and is [one of the four types](https://en.wikipedia.org/wiki/Flynn%27s_taxonomy) of parallel processing.
SIMD is distinct from the typical way we think about parallel code i.e. a multi-processor architecture where each core runs its own set of instructions on its own data, "multiple instruction, multiple data (MIMD)."
In the case of SIMD we want to apply the same operation to lots of data.
This is normally done thought vectorisation of the program, using specific instructions that leverage specialised hardware, we can apply the same instruction to a vector (an array) of data.

Imagine we have a vector, and we want to add 1 to every element. 
In a classic process we would have to loop over every element e.g:

```python
for i in range(len(vector))
    vector[i] += 1
```
In this case the processor would update each element in the vector one after the other. However using `jax.vmap` we can vectorise this operation:

```python
jax.vmap(lambda x: x+1)(vector)
```

By using SIMD rather than the operation taking `len(vector)` cycles, using the correct instructions and hardware, it can be done in a single cycle.
Modern CPUs have support for vector operations with their AVX instruction set, but GPUs are much better.
And thanks to libraries like JAX it's, at times, trivially easy to convert code to SIMD.

How does this relate to deep learning? 
Well all those fancy models are mostly just made up of a lot of operations that look something like $$X \cdot W + b$$ over vary big matrices.
This is something that GPUs can parallelise very well. 

## Speeding up GAs
So what, does SIMD have to do with GAs?
In short, we can use JAX to offload the work to the GPU and `jax.vmap` SIMD framework to maximise the GPUs usage.

My first step was to `jax.jit` compile the GA so that it ran on the GPU.
However, doing so had a minimal improvement on performance.
Running some performance analysis GPU usage was low.
Based on Will Whitney [blog post](https://willwhitney.com/parallel-training-jax.html) my idea was to use `jax.vmap`.
The easy change is to apply `vmap` to fitness function `fitness = jax.vmap(fit_fn)(population)`.
For the crossover and mutate its a little more involved because of the way JAX deals with random numbers, I ended up using a `scan`.

```python
# From the above GA
candidates = best_half

def make_new_pop(nkey: PRNGKeyArray, _) -> Tuple[PRNGKeyArray, Solution]:
    mikey, mukey, nkey = jax.random.split(nkey, 3)
    pars = jax.random.choice(key, candidates, replace=False, shape=(2,))
    child = self.mix(mikey, pars[1], pars[1])
    child = self.mutate(mukey, child, rate=mutate_rate)
    return nkey, child

_, new_pop = jax.lax.scan(make_new_pop, init=nkey, xs=None, length=self.population_size // 2)
population = jax.stack([candidates, new_pop])
```

With these changes, the GA runs on GPU and makes good use of it. 
There is still a fair bit of overhead with the `jax.jit` compilations.
However, the purpose of this was to run a GA where the fitness function is conditioned on some input values.
Using another layer of `vmap`s I was able to get it to run for 1000 samples, 1000 generations with a population of 128 in about 5 mins.

## The Caveats
As I alluded to, there are a few limitations to this approach:

1. The objective function must be `jax.jit` compilable. This is the biggest limitation. While JAX is very expressive and anything that can be written using numpy should work fine. I can see there being issues with existing code / complex simulations.
2. Because of limitations with how `jax.vmap` and `jax.scan` use memory there are limits on the population size/number of generations. However, with some changes, it should be possible to get around these issues.






